{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb3c4b1-eebb-4fc6-b9a0-9fe38a19bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Base: 141 Nev., Advance Opinion c9a\n",
      "IN THE SUPREME COURT OF THE STATE OF NEVADA\n",
      "QUASHAWN SAQUAN SHERIDAN,                               No.\n",
      "Cümle 1 - Lemmatized: ['advance', 'opinion', 'supreme', 'court', 'state', 'nevada', 'quashawn', 'saquan', 'sheridan']\n",
      "Cümle 1 - Stemmed: ['advanc', 'opinion', 'suprem', 'court', 'state', 'nevada', 'quashawn', 'saquan', 'sheridan']\n",
      "\n",
      "\n",
      "Cümle 2 - Base: 89167\n",
      "Appellant,\n",
      "vs.\n",
      "Cümle 2 - Lemmatized: ['appellant', 'v']\n",
      "Cümle 2 - Stemmed: ['appel', 'vs']\n",
      "\n",
      "\n",
      "Cümle 3 - Base: THE STATE OF NEVADA,                                a    FILED\n",
      "Respondent.\n",
      "Cümle 3 - Lemmatized: ['state', 'nevada', 'filed', 'respondent']\n",
      "Cümle 3 - Stemmed: ['state', 'nevada', 'file', 'respond']\n",
      "\n",
      "\n",
      "Cümle 4 - Base: APR 2 4 2ŒZ5\n",
      ":YL;Mr\n",
      "HA               RT\n",
      "Appeal from a district court order revoking appellant's\n",
      "probation.\n",
      "Cümle 4 - Lemmatized: ['apr', 'yl', 'mr', 'ha', 'rt', 'appeal', 'district', 'court', 'order', 'revoking', 'appellant', 'probation']\n",
      "Cümle 4 - Stemmed: ['apr', 'yl', 'mr', 'ha', 'rt', 'appeal', 'district', 'court', 'order', 'revok', 'appel', 'probat']\n",
      "\n",
      "\n",
      "Cümle 5 - Base: Second Judicial District Court, Washoe County; Tammy Riggs,\n",
      "Judge.\n",
      "Cümle 5 - Lemmatized: ['second', 'judicial', 'district', 'court', 'washoe', 'county', 'tammy', 'riggs', 'judge']\n",
      "Cümle 5 - Stemmed: ['second', 'judici', 'district', 'court', 'washo', 'counti', 'tammi', 'rigg', 'judg']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Gerekli NLTK bileşenlerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Stopwords listesi ve yardımcı nesneler\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Dosyayı oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Her cümle için lemmatize ve stem işlemleri\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "for sentence in sentences:\n",
    "    lemmatized, stemmed = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized)\n",
    "    tokenized_corpus_stemmed.append(stemmed)\n",
    "\n",
    "# CSV dosyasına kaydet (lemmatized)\n",
    "with open(\"lemmatized_sentences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])\n",
    "\n",
    "# CSV dosyasına kaydet (stemmed)\n",
    "with open(\"stemmed_sentences.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])\n",
    "\n",
    "# İlk 5 cümleyi yazdır\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"Cümle {i+1} - Base: {sentences[i]}\")\n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132930c1-12df-49d2-bb03-9bf7500a45e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
