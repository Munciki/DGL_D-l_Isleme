{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b35e5a2-e783-4ec1-99d4-b5304deb10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aaron  absconding  absent  abuse  act  acting  administrative  \\\n",
      "0    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "1    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "2    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "3    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "4    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "\n",
      "   administratively  admitted  advance  ...  violation  violence  violent  \\\n",
      "0               0.0       0.0   0.4349  ...        0.0       0.0      0.0   \n",
      "1               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "2               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "3               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "4               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "\n",
      "   walk   washoe  williams  within  without        yl   ôl  \n",
      "0   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "1   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "2   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "3   0.0  0.00000       0.0     0.0      0.0  0.343143  0.0  \n",
      "4   0.0  0.32753       0.0     0.0      0.0  0.000000  0.0  \n",
      "\n",
      "[5 rows x 310 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Gerekli bileşenleri indir (ilk kez çalıştıracaksan)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Dosyayı oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# İngilizce stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Lemmatize ve stem yapılmış corpus\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lem, stem = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lem)\n",
    "    tokenized_corpus_stemmed.append(stem)\n",
    "\n",
    "# TF-IDF için metinleştirme\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörleştirme\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Kelime isimleri (sütunlar)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame'e çevir\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk 5 satırı göster\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06977c9-6778-4998-bb42-c6bd753ce7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'justice' kelimesi TF-IDF vektörleri arasında bulunamadı.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Gerekli NLTK verilerini indir (ilk çalıştırmada gerekebilir)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# TXT dosyasından metni oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Temizleme ve önişleme fonksiyonu\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Cümleleri temizle\n",
    "tokenized_corpus = [' '.join(preprocess_sentence(sentence)) for sentence in sentences]\n",
    "\n",
    "# TF-IDF vektörizasyonu\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(tokenized_corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame formatında görelim (istersen)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# === Cosine Similarity Hesaplama === #\n",
    "target_word = \"justice\"  # İstediğin kelimeyi burada değiştir\n",
    "\n",
    "if target_word in feature_names:\n",
    "    target_index = list(feature_names).index(target_word)\n",
    "    target_vector = tfidf_matrix[:, target_index].toarray()\n",
    "    all_vectors = tfidf_matrix.toarray()\n",
    "    similarities = cosine_similarity(target_vector.T, all_vectors.T).flatten()\n",
    "\n",
    "    # En yakın 5 kelimeyi bulalım (kendisi dahil, bu yüzden 6 alıp kendini dışlayacağız)\n",
    "    top_indices = similarities.argsort()[-6:][::-1]\n",
    "    print(f\"\\nKelime: '{target_word}' ile en çok ilişkili 5 kelime:\")\n",
    "    for idx in top_indices:\n",
    "        if feature_names[idx] != target_word:\n",
    "            print(f\"{feature_names[idx]}: {similarities[idx]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi TF-IDF vektörleri arasında bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2531bd6-5a7d-41b5-b171-6b436cfab74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'justice' kelimesi metinde yer almıyor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Gerekli NLTK verileri\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Metni oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Tüm cümleleri temizle\n",
    "processed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# TF-IDF hesapla\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# === COSINE SIMILARITY === #\n",
    "target_word = \"justice\"  # Burayı istediğin kelime ile değiştir: 'law', 'lawyer', 'court' vs.\n",
    "\n",
    "if target_word in feature_names:\n",
    "    word_index = feature_names.tolist().index(target_word)\n",
    "    word_vector = tfidf_matrix[:, word_index].toarray()\n",
    "\n",
    "    # Tüm kelime vektörlerini al\n",
    "    all_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "    # Cosine similarity hesapla\n",
    "    similarities = cosine_similarity(word_vector.T, all_vectors.T).flatten()\n",
    "\n",
    "    # En çok benzeyen 5 kelimeyi seç (kendisi dahil olduğu için 6 alıp 1 dışlayacağız)\n",
    "    top_indices = similarities.argsort()[-6:][::-1]\n",
    "\n",
    "    print(f\"\\n'{target_word}' kelimesiyle en çok ilişkili 5 kelime:\")\n",
    "    for idx in top_indices:\n",
    "        if feature_names[idx] != target_word:\n",
    "            print(f\"{feature_names[idx]}: {similarities[idx]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi metinde yer almıyor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69e2f0a-4b7b-41d6-b288-2b0de042b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'justice' kelimesiyle en çok ilişkili 5 kelime:\n",
      "the: 0.7859\n",
      "system: 0.6650\n",
      "and: 0.5964\n",
      "law: 0.5877\n",
      "people: 0.5065\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Örnek cümleler — test amaçlı, içinde justice geçiyor\n",
    "sample_sentences = [\n",
    "    \"The justice system must be fair and impartial.\",\n",
    "    \"Every citizen deserves equal justice under the law.\",\n",
    "    \"A lawyer fights for justice in court.\",\n",
    "    \"Law and order are essential to maintain justice.\",\n",
    "    \"Many people trust the justice system.\"\n",
    "]\n",
    "\n",
    "# TF-IDF vektörleştirme\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sample_sentences)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Anahtar kelime\n",
    "target_word = \"justice\"\n",
    "\n",
    "# Eğer kelime TF-IDF'de varsa\n",
    "if target_word in feature_names:\n",
    "    target_index = feature_names.tolist().index(target_word)\n",
    "    word_vector = tfidf_matrix[:, target_index].toarray()\n",
    "    all_vectors = tfidf_matrix.toarray()\n",
    "    similarities = cosine_similarity(word_vector.T, all_vectors.T).flatten()\n",
    "\n",
    "    top_indices = similarities.argsort()[-6:][::-1]\n",
    "\n",
    "    print(f\"\\n'{target_word}' kelimesiyle en çok ilişkili 5 kelime:\")\n",
    "    for idx in top_indices:\n",
    "        if feature_names[idx] != target_word:\n",
    "            print(f\"{feature_names[idx]}: {similarities[idx]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi metinde yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a363fc-ca73-4fb7-ab6c-f41a800a2005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'justice' kelimesi metinde yok.\n",
      "\n",
      "TF-IDF Matrisi (ilk 5 satır):\n",
      "   aaron  absconding  absent  abuse  act  acting  administrative  \\\n",
      "0    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "1    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "2    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "3    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "4    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "\n",
      "   administratively  admitted  advance  ...  violation  violence  violent  \\\n",
      "0               0.0       0.0   0.4349  ...        0.0       0.0      0.0   \n",
      "1               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "2               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "3               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "4               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "\n",
      "   walk   washoe  williams  within  without        yl   ôl  \n",
      "0   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "1   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "2   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "3   0.0  0.00000       0.0     0.0      0.0  0.343143  0.0  \n",
      "4   0.0  0.32753       0.0     0.0      0.0  0.000000  0.0  \n",
      "\n",
      "[5 rows x 310 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "# Gerekli bileşenleri indir (ilk kez çalıştıracaksan)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Dosyayı oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# İngilizce stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Lemmatize ve stem yapılmış corpus\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lem, stem = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lem)\n",
    "    tokenized_corpus_stemmed.append(stem)\n",
    "\n",
    "# TF-IDF için metinleştirme\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# TF-IDF vektörleştirme\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Kelime isimleri (sütunlar)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame'e çevir\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Rasgele 5 cümle seç\n",
    "random_sentences = random.sample(sentences, 5)\n",
    "\n",
    "# Anahtar kelime\n",
    "target_word = \"justice\"  # Burada istediğiniz kelimeyi yazabilirsiniz\n",
    "\n",
    "# TF-IDF vektörleştirme\n",
    "tfidf_matrix_random = vectorizer.fit_transform(random_sentences)\n",
    "feature_names_random = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Eğer kelime TF-IDF'de varsa\n",
    "if target_word in feature_names_random:\n",
    "    target_index = feature_names_random.tolist().index(target_word)\n",
    "    word_vector = tfidf_matrix_random[:, target_index].toarray()\n",
    "    all_vectors = tfidf_matrix_random.toarray()\n",
    "    similarities = cosine_similarity(word_vector.T, all_vectors.T).flatten()\n",
    "\n",
    "    top_indices = similarities.argsort()[-6:][::-1]\n",
    "\n",
    "    print(f\"\\n'{target_word}' kelimesiyle en çok ilişkili 5 kelime:\")\n",
    "    for idx in top_indices:\n",
    "        if feature_names_random[idx] != target_word:\n",
    "            print(f\"{feature_names_random[idx]}: {similarities[idx]:.4f}\")\n",
    "else:\n",
    "    print(f\"'{target_word}' kelimesi metinde yok.\")\n",
    "\n",
    "# İlk 5 satırı göster\n",
    "print(\"\\nTF-IDF Matrisi (ilk 5 satır):\")\n",
    "print(tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0862bb-5520-4e8c-8a40-557ee9074432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 anlamlı cümle ile metindeki benzerlik analizi:\n",
      "\n",
      "Benzerlik 0.3989: But when, as\n",
      "here,   the   district   court's   determination   was    based   on   statutory\n",
      "interpretation, we review the district court's decision de novo.\n",
      "\n",
      "Benzerlik 0.3140: DISCUSSION\n",
      "A district court's decision to revoke probation is within its broad\n",
      "discretion and will not be disturbed absent a clear showing of abuse.\n",
      "\n",
      "Benzerlik 0.1630: BEFORE THE        SUPREME      COURT,    PARRAGUIRRE.\n",
      "\n",
      "Benzerlik 0.1445: SUPREME COURT\n",
      "OF\n",
      "NEVADA\n",
      "1 g 2-9\n",
      "0) 1947A    e\n",
      "OPINION\n",
      "By the Court, PARRAGUIRRE, J.:\n",
      "NRS 176A.510 requires the imposition of graduated sanctions\n",
      "for technical probation violations.\n",
      "\n",
      "Benzerlik 0.1420: Because the district court failed to support its findings with facts\n",
      "showing that Sheridan's convictions constitute crimes of violence\n",
      "amounting to nontechnical probation violations, we reverse the district\n",
      "court's revocation of Sheridan's probation and remand this matter to the\n",
      "district court for further proceedings.\n",
      "\n",
      "TF-IDF Matrisi (ilk 5 satır):\n",
      "   aaron  absconding  absent  abuse  act  acting  administrative  \\\n",
      "0    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "1    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "2    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "3    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "4    0.0         0.0     0.0    0.0  0.0     0.0             0.0   \n",
      "\n",
      "   administratively  admitted  advance  ...  violation  violence  violent  \\\n",
      "0               0.0       0.0   0.4349  ...        0.0       0.0      0.0   \n",
      "1               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "2               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "3               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "4               0.0       0.0   0.0000  ...        0.0       0.0      0.0   \n",
      "\n",
      "   walk   washoe  williams  within  without        yl   ôl  \n",
      "0   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "1   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "2   0.0  0.00000       0.0     0.0      0.0  0.000000  0.0  \n",
      "3   0.0  0.00000       0.0     0.0      0.0  0.343143  0.0  \n",
      "4   0.0  0.32753       0.0     0.0      0.0  0.000000  0.0  \n",
      "\n",
      "[5 rows x 310 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Gerekli NLTK bileşenlerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Karar metnini oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# İngilizce stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Metni işle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lem, stem = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lem)\n",
    "    tokenized_corpus_stemmed.append(stem)\n",
    "\n",
    "# TF-IDF için metinleştirme\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "# Vektörleştirme\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "# Kelime isimleri (özellik adları)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# DataFrame'e dönüştür\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Anlamlı 5 cümle (örnek)\n",
    "sample_sentences = [\n",
    "    \"The court's decision reflects a deep understanding of the legal principles involved.\",\n",
    "    \"Justice should be served in every case, ensuring fairness and equality.\",\n",
    "    \"The defendant's actions were deemed illegal by the court based on the evidence presented.\",\n",
    "    \"In legal matters, it is crucial to uphold the rule of law for a just society.\",\n",
    "    \"The judge ruled that the evidence was sufficient to convict the defendant of the crime.\"\n",
    "]\n",
    "\n",
    "# Örnek cümleleri dönüştür (vectorizer.fit zaten yapıldı)\n",
    "tfidf_matrix_sample = vectorizer.transform(sample_sentences)\n",
    "\n",
    "# Cosine Similarity hesapla\n",
    "similarities = cosine_similarity(tfidf_matrix_sample, tfidf_matrix)\n",
    "\n",
    "# En çok benzeyen cümlelerin indeksleri\n",
    "top_indices = similarities[0].argsort()[-5:][::-1]\n",
    "\n",
    "# Benzerlik sonuçlarını yazdır\n",
    "print(f\"\\n5 anlamlı cümle ile metindeki benzerlik analizi:\")\n",
    "for idx in top_indices:\n",
    "    similarity_value = float(similarities[0, idx])  # HATA BURADA DÜZELTİLDİ\n",
    "    print(f\"\\nBenzerlik {similarity_value:.4f}: {sentences[idx]}\")\n",
    "\n",
    "# İlk 5 satır TF-IDF matrisi\n",
    "print(\"\\nTF-IDF Matrisi (ilk 5 satır):\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2d688c-1d40-4862-9db6-fb5544422f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV dosyaları başarıyla oluşturuldu: TD-IDF.lemma.csv ve TD-IDF.stem.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Gerekli bileşenleri indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Dosyayı oku\n",
    "with open(\"karar_metni_2025-05-03_16-17-56.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Cümlelere ayır\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatizer ve stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# İngilizce stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Lemmatize ve stem yapılmış corpus\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    lem, stem = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lem)\n",
    "    tokenized_corpus_stemmed.append(stem)\n",
    "\n",
    "# TF-IDF için metinleştirme\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "\n",
    "# Lemmatized TF-IDF\n",
    "vectorizer_lem = TfidfVectorizer()\n",
    "tfidf_matrix_lem = vectorizer_lem.fit_transform(lemmatized_texts)\n",
    "feature_names_lem = vectorizer_lem.get_feature_names_out()\n",
    "tfidf_df_lem = pd.DataFrame(tfidf_matrix_lem.toarray(), columns=feature_names_lem)\n",
    "\n",
    "# Stemmed TF-IDF\n",
    "vectorizer_stem = TfidfVectorizer()\n",
    "tfidf_matrix_stem = vectorizer_stem.fit_transform(stemmed_texts)\n",
    "feature_names_stem = vectorizer_stem.get_feature_names_out()\n",
    "tfidf_df_stem = pd.DataFrame(tfidf_matrix_stem.toarray(), columns=feature_names_stem)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "tfidf_df_lem.to_csv(\"TD-IDF.lemma.csv\", index=False)\n",
    "tfidf_df_stem.to_csv(\"TD-IDF.stem.csv\", index=False)\n",
    "\n",
    "print(\"CSV dosyaları başarıyla oluşturuldu: TD-IDF.lemma.csv ve TD-IDF.stem.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fff26e-ee78-4077-8450-1db53773b540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
